{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advance_Computer_Vision_with_CNN_R__FaceRecognition",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhlRk1TVkNxi"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvWl3ebqzCc1"
      },
      "source": [
        "# Instructions\n",
        "- Some parts of the code are already done for you\n",
        "- You need to execute all the cells\n",
        "- You need to add the code where ever you see `\"#### Add your code here ####\"`\n",
        "- Marks are mentioned along with the cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgR0j5310qqC"
      },
      "source": [
        "# Face recognition\n",
        "Task is to recognize a faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_f3HHLmJIuT"
      },
      "source": [
        "### Dataset\n",
        "**Aligned Face Dataset from Pinterest**\n",
        "\n",
        "This dataset contains 10.770 images for 100 people. All images are taken from 'Pinterest' and      aligned using dlib library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV3hpS9ciSFr",
        "outputId": "d469e469-5ed4-4c44-f0e3-a140ffa9964a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjRTlPkp1LC2"
      },
      "source": [
        "#### Mount Google drive if you are using google colab\n",
        "- We recommend using Google Colab as you can face memory issues and longer runtimes while running on local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBWMoTJ9cf3Z",
        "outputId": "e14bdeb9-ea80-4ad5-f41a-51645792e508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO9mgMmp13sI"
      },
      "source": [
        "#### Change current working directory to project folder (2 mark)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TddMnf4D1-59",
        "outputId": "8073f3b1-2ed5-49e7-cff9-e5d924219630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/GL-AIML/ACV Lab Questions and Data Set')\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Advance Computer Vision with CNN_R_Project2_FaceRecognition.ipynb.ipynb'\n",
            "'Aligned Face Dataset.zip'\n",
            " Bounding_Box_Class_version.ipynb\n",
            " images\n",
            " images_racoon.rar\n",
            " pins\n",
            " PINS\n",
            "'R9 - ACV - Internal_Lab_Questions.ipynb'\n",
            " train_labels.csv\n",
            " vgg_face_weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBB_OncAQ8h_"
      },
      "source": [
        "### Extract the zip file (4 marks)\n",
        "- Extract Aligned Face Dataset from Pinterest.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI5uhBunLEZ9"
      },
      "source": [
        "#### Add your code here ####\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('Aligned Face Dataset.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKK2o2tScdKQ",
        "outputId": "62bd73fd-8ca6-4d5c-e650-4eba514854ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/GL-AIML/ACV Lab Questions and Data Set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oesXJD9ySB6w"
      },
      "source": [
        "### Function to load images\n",
        "- Define a function to load the images from the extracted folder and map each image with person id \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q7TS19vVbGb"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class IdentityMetadata():\n",
        "    def __init__(self, base, name, file):\n",
        "        # print(base, name, file)\n",
        "        # dataset base directory\n",
        "        self.base = base\n",
        "        # identity name\n",
        "        self.name = name\n",
        "        # image file name\n",
        "        self.file = file\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.image_path()\n",
        "\n",
        "    def image_path(self):\n",
        "        return os.path.join(self.base, self.name, self.file) \n",
        "    \n",
        "def load_metadata(path):\n",
        "    metadata = []\n",
        "    for i in os.listdir(path):\n",
        "        for f in os.listdir(os.path.join(path, i)):\n",
        "            # Check file extension. Allow only jpg/jpeg' files.\n",
        "            ext = os.path.splitext(f)[1]\n",
        "            if ext == '.jpg' or ext == '.jpeg':\n",
        "                metadata.append(IdentityMetadata(path, i, f))\n",
        "    return np.array(metadata)\n",
        "\n",
        "# metadata = load_metadata('images')\n",
        "metadata = load_metadata('PINS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1Vzl3MPebA"
      },
      "source": [
        "### Define function to load image\n",
        "- Define a function to load image from the metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ape5WxvVWKOe"
      },
      "source": [
        "import cv2\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path, 1)\n",
        "    # OpenCV loads images with color channels\n",
        "    # in BGR order. So we need to reverse them\n",
        "    return img[...,::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYm-aYUDRANv"
      },
      "source": [
        "#### Load a sample image (4 marks)\n",
        "- Load one image using the function \"load_image\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptDNq8noWK89"
      },
      "source": [
        "#### Add your code here ####\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "image19 = str(metadata[19])\n",
        "plt.imshow(load_image(image19))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg0olr-8Xbqw"
      },
      "source": [
        "### VGG Face model\n",
        "- Here we are giving you the predefined model for VGG face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh0Pz6acuaDP"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n",
        "\n",
        "def vgg_face():\t\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    \n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    \n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    \n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    \n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Convolution2D(2622, (1, 1)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2JhG4NOe7vd"
      },
      "source": [
        "#### Load the model (4 marks)\n",
        "- Load the model defined above\n",
        "- Then load the given weight file named \"vgg_face_weights.h5\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAa3OASPvKac"
      },
      "source": [
        "#### Add your code here ####\n",
        "model = vgg_face()\n",
        "model.load_weights('vgg_face_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mStdpxzAf7y5"
      },
      "source": [
        "### Get vgg_face_descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9IQ9hcSwO9k"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkBQRL_sd2U8"
      },
      "source": [
        "### Generate embeddings for each image in the dataset\n",
        "- Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2yd69OydBAq",
        "outputId": "0e446ad3-0539-4523-ac44-e9d6bb9a5706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get embedding vector for first image in the metadata using the pre-trained model\n",
        "\n",
        "img_path = metadata[0].image_path()\n",
        "img = load_image(img_path)\n",
        "\n",
        "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
        "img = (img / 255.).astype(np.float32)\n",
        "\n",
        "img = cv2.resize(img, dsize = (224,224))\n",
        "print(img.shape)\n",
        "\n",
        "# Obtain embedding vector for an image\n",
        "# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \n",
        "\n",
        "embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n",
        "print(embedding_vector.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(224, 224, 3)\n",
            "(2622,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plHvUTytcTGo"
      },
      "source": [
        "### Generate embeddings for all images (10 marks)\n",
        "- Write code to iterate through metadata and create embeddings for each image using `vgg_face_descriptor.predict()` and store in a list with name `embeddings`\n",
        "\n",
        "- If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 2622-zeroes as the final embedding from the model is of length 2622."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY9ykxtueY4k",
        "outputId": "fa679a3a-7c97-4ee3-8b07-65e074ae2a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "embeddings = np.zeros((metadata.shape[0], 2622))\n",
        "\n",
        "for i, m in enumerate(metadata):\n",
        "  #### Add your code here ####\n",
        "  img_path = m.image_path()\n",
        "  img = load_image(img_path)\n",
        "  img = (img / 255.).astype(np.float32) # Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
        "  img = cv2.resize(img, dsize = (224,224))\n",
        "  embeddings[i] = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5638ce2cd216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#### Add your code here ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6d96fb74d85b>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# OpenCV loads images with color channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# in BGR order. So we need to reverse them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Proo1auf6uf"
      },
      "source": [
        "print('Size of embedding: ', len(embeddings[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hb3XSDsfTMG"
      },
      "source": [
        "### Function to calculate distance between given 2 pairs of images.\n",
        "\n",
        "- Consider distance metric as \"Squared L2 distance\"\n",
        "- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sNnRtt-U7aU"
      },
      "source": [
        "def distance(emb1, emb2):\n",
        "    return np.sum(np.square(emb1 - emb2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwVRkeoNUyUw"
      },
      "source": [
        "#### Plot images and get distance between the pairs given below\n",
        "- 2, 3 and 2, 180\n",
        "- 30, 31 and 30, 100\n",
        "- 70, 72 and 70, 115"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDVLED10eboB"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_pair(idx1, idx2):\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(load_image(metadata[idx1].image_path()))\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(load_image(metadata[idx2].image_path()));    \n",
        "\n",
        "show_pair(2, 3)\n",
        "show_pair(2, 180)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G2iDeWKYMae"
      },
      "source": [
        "### Create train and test sets (10 marks)\n",
        "- Create X_train, X_test and y_train, y_test\n",
        "- Use train_idx to seperate out training features and labels\n",
        "- Use test_idx to seperate out testing features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OThdBDPxYkd4"
      },
      "source": [
        "train_idx = np.arange(metadata.shape[0]) % 9 != 0     #every 9th example goes in test data and rest go in train data\n",
        "test_idx = np.arange(metadata.shape[0]) % 9 == 0\n",
        "\n",
        "# one half as train examples of 10 identities\n",
        "X_train = embeddings[train_idx]\n",
        "# another half as test examples of 10 identities\n",
        "\n",
        "#### Add your code here ####\n",
        "X_test = embeddings[test_idx]\n",
        "\n",
        "targets = np.array([m.name for m in metadata])\n",
        "#train labels\n",
        "y_train = targets[train_idx]\n",
        "#test labels\n",
        "#### Add your code here ####\n",
        "y_test = y[test_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlYYwGQxXVwf"
      },
      "source": [
        "### Encode the Labels (6 marks)\n",
        "- Encode the targets\n",
        "- Use LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GOQrjqeX2LZ"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#### Add your code here ####\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9CylOWOa4xM"
      },
      "source": [
        "### Standardize the feature values (6 marks)\n",
        "- Scale the features using StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7pUV0oYbLrR"
      },
      "source": [
        "# Standarize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#### Add your code here ####\n",
        "standard_scaler = StandardScaler()\n",
        "X_train_scaled = standard_scaler.fit_transform(X_train)\n",
        "X_test_scaled = standard_scaler.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2QukHGXbb6d"
      },
      "source": [
        "### Reduce dimensions using PCA (6 marks)\n",
        "- Reduce feature dimensions using Principal Component Analysis\n",
        "- Set the parameter n_components=128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVj1SSEebtG8"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#### Add your code here ####\n",
        "pca = PCA(.96)\n",
        "pca.fit(X_train_scaled)\n",
        "X_train_pca = pca.transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Checking total components\n",
        "pca.n_components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzCsmZg8chW4"
      },
      "source": [
        "### Build a Classifier (6 marks)\n",
        "- Use SVM Classifier to predict the person in the given image\n",
        "- Fit the classifier and print the score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnBv9Ks0cwtA"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "#### Add your code here ####\n",
        "clf = SVC()\n",
        "clf.fit(X_train_pca, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V2MvD3Dk2rO"
      },
      "source": [
        "# Acc on training data\n",
        "clf.score(X_train_pca, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-5Gt_5Ek5Y5"
      },
      "source": [
        "# Acc On test data\n",
        "clf.score(X_test_pca, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGz1G8e3dUl5"
      },
      "source": [
        "### Test results (2 mark)\n",
        "- Take 10th image from test set and plot the image\n",
        "- Report to which person(folder name in dataset) the image belongs to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zD_f8Sudeiw"
      },
      "source": [
        "import warnings\n",
        "# Suppress LabelEncoder warning\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "example_idx = 10\n",
        "\n",
        "example_image = load_image(metadata[test_idx][example_idx].image_path())\n",
        "example_prediction = #### Add your code here ####\n",
        "example_identity = encoder.inverse_transform(example_prediction)[0]\n",
        "\n",
        "plt.imshow(example_image)\n",
        "plt.title(f'Identified as {example_identity}');"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}